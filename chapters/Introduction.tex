% !TEX root = ../thesis.tex

\chapter{Introduction}
\label{chap:intro}
\section{Motivation}
\label{sec:motiv}

Technology is becoming evermore ubiquitous. In its ubiquity this technology brings many things, but above all it has brought a rise in the supply of data and the value of time and attention, both from a consumer and a business perspective. Here machine learning might provide a solution since, what one might call, ``manual automatization'', i.e. explicitly programming a computer to make certain kinds of decisions, is complex, labour intensive, highly specific and thus expensive and often ineffective. Machine learning however teaches quite literally by example. As long as one has the data, by way of machine learning one can teach computers to do things beyond one's own abilities. There are various forms of machine learning, but one of the most common, and the one with which we will be primarily concerned in this thesis, is \textit{classification}. This type of learning will attempt to label a given input based on previous examples. While better than ``manual automatization'' general machine learning can still be difficult since it depends on criteria and values that might not be obvious. Here we can again provide a solution: boosting, a way to turn a ``good enough'' and usually obvious criterium into a very good one.

\section{Outline}
\subsection{setting}
There are three contexts with which we will be concerned in this thesis: classification, boosting and hedging.
Classification is a setting in which we are given access to certain example ``problems '' and their ``correct answers''. The goal is to find a way to answer future problems correctly as best possible. Our second context is boosting. This is an answer to the classification problems and comes down to the following: we want to find a rule of thumb that performs only slightly better than random guessing and ``training''  that rule in such a way that the rule of thumb  can distinguish between important and irrelevant aspects of the problem and thus perform better. The final context is that of hedging. Meaning ``to protect oneself against loss by making balancing or compensating transactions.'', hedging is all about cleverly allocating resources to minimize losses. Hedging algorithms are often used as subroutines of boosting algorithms to determine the relevant aspects of a problem as mentioned previously. 
\newpage
\subsection{Scope}
The main goal of this thesis is to compare the practical performance of three algorithms: \adaB, \NHB and \squintB. These are boosting algorithms based on hedge allocation algorithms called \hedge\footnote{The algorithm was originally called \textbf{Hedge}$(\beta)$ but we use a different parametrization here and adjusted the name accordingly}, \adaN and \squint respectively. In \cite{Koolen2015} Koolen and Van Erven proved that \squint theoretically outperforms \hedge and \adaN. However since the theory must account for edge cases which might not occur in the average scenario it is possible that the difference in practical performance between \squint and the other two algorithms is negligible or even negative(that is that \squintB might not outperform the other two in practice). In this thesis we will implement all of the above algorithms to compare their accuracy as a function of the size of the training data and the number of iterations. We will also compare them computationally, analysing their ratio of computation time to accuracy.  

\subsection{Basic structure}
In chapter 2 we will give a formal description of the settings and procedures we will use throughout the thesis. We will also introduce the notations and basic terminology, as well as briefly discussing the results of previous works, both practical and theoretical. In chapter 3 we will discuss the results of the algorithms and compare their accuracy. Finally in chapter 4 we will discuss the computational performance of the algorithms    
