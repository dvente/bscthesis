% !TEX root = ../thesis.tex

\chapter{Introduction}
\label{chap:intro}
\section{Motivation}
\label{sec:motiv}

Technology is becoming evermore ubiquitous. In its ubiquity this technology brings many things, but above all it has brought a rise in the supply of data and the value of time and attention, both from a consumer and a business perspective. Here machine learning might provide a solution since, what one might call, ``manual automatization'', i.e. explicitly programming a computer to make certain kinds of decisions, is complex, labour intensive, highly specific and thus expensive and often ineffective. Machine learning however teaches quite literally by example. As long as one has the data, by way of machine learning one can teach computers to do things beyond one's own abilities. While better than ``manual automatization'' general machine learning can still be hard since it depends on criteria and values that might not be obvious. Here we can again provide a solution: boosting, a way to take a ``good enough'' and usually obvious criterium into a very good one.

\section{Scope} Here we will consider and compare three such algorithms: \adaB, \adaN and \textbf{Squint}. In this thesis we will examine their relative performance in practice. It has already been proven that \squint performs significantly better then the other two in theory. This can however be different in practice since the theory must account for edge cases which might not show up in the average use. In this paper we will implement, examine and compare the three algorithems and their accuracy in terms of the size of their training set. (optional: we will also compare them on the computational side, seeing how the performance to computation ratio will turn out). Below we will give a brief introduction to the three algorithms, after briefly introducing \hedge and \weak which form the basis for \adaB and \adaN.

\section{\weak}
The idea of boosting is to combine one or more strategies that perform at least slightly better then random guessing, into a good rule to predict some outcome. Such strategies are called weak learners. In this paper we will primarily consider what are called decision stumps. These weak learners measure one aspect of the to classify example and then output a decision. Imagine for example that we are trying to decide whether an email is spam or not. An example of a decision stump would be to count the number of \$ in the text and if there are more than 5 we label it as spam and otherwise as genuine. 

\newpage \section{\hedge}
In their paper Freund and Schapire introduced \hedge as an allocation algorithm to be able to combine several strategies into one. Consider the following example: When betting on a horse race one might have several rules of thumb. Using \hedge one can distribute ones betting money in such a way that the loss after several rounds will be ``not much worse'' then if one had bet everything according to the best rule of thumb. This generalized previous allocation algorithms since this one was not only applicable to multi class classification but also doesn't require any prior knowledge of the bias of the rules of thumb which was essential to the previous algorithms. A disadvantage of \hedge is however that the losses it achieves depended on a parameter $\beta$ which has to be chosen prior to running the algorithm. While this allows for the exploitation of any prior knowledge one might have it is often not known ahead of time how to choose $\beta$ optimally. Using this \hedge algorithm Freund and Schapire developed \adaB, a application of \hedge in the context of boosting, which we will discuss below. 

\section{\adaB}
\adaB is a learning algorithm, which means that provided with enough examples with a correct answer attached, \adaB will attempt to formulate a hypothesis, i.e. a rule to label future inputs with, that minimalizes the error made on the new examples. Where \hedge is about using several different strategies, \adaB uses one. One of the most notable differences between the way \hedge and \adaB operate is that \adaB does not require a parameter prior to execution like \hedge does but produces it internally, eliminating more of the need for prior knowledge. Another notable difference is that \hedge increases the weights of the strategies that perform well while \adaB does the exact opposite and thus tries to ``focus'' the weak learning algorithm on the hard examples. 
