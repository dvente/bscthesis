% !TEX root = ../thesis.tex

\chapter{Introduction}
\label{chap:intro}
\section{Motivation}
\label{sec:motiv}

Technology is becoming evermore ubiquitous. In its ubiquity this technology brings many things, but above all it has brought a rise in the supply of data and the value of time and attention, both from a consumer and a business perspective. Here machine learning might provide a solution since, what one might call, ``manual automatization'', i.e. explicitly programming a computer to make certain kinds of decisions, is complex, labour intensive, highly specific and thus expensive and often ineffective. Machine learning however teaches quite literally by example. As long as one has the data, by way of machine learning one can teach computers to do things beyond one's own abilities. There are various forms of machine learning, but one of the most common, and the one with which we will be primarily concerned in this thesis, is \textit{classification}. This type of learning will attempt to label a given input based on previous examples. While better than ``manual automatization'' general machine learning can still be difficult since it depends on criteria and values that might not be obvious. Here we can again provide a solution: boosting, a way to turn a ``good enough'' and usually obvious criterium into a very good one.

\section{Scope and outline}
There are three contexts with which we will be concerned in this thesis: classification, boosting and hedging.
Classification, sometimes also called \textit{sequential online prediction}, is a setting in which we are given access to certain example ``problems '' and their ``correct answers''. After studying them we must then be able to predict the answer of future problems. Our second context is boosting. This is an answer to the classification problems and comes down to finding a rule of thumb that performs only slightly better than random guessing and ``training''  that rule in such a way that 
% Here we will consider and compare three algorithms: \adaB, \NHB and \textbf{Squint-Boost}. In this thesis we will examine their relative performance in practice. It has theoretically been proven that \squint performs significantly better than the other two. This can however be different in practice since the theory must account for edge cases which might not show up in the average use. In this thesis we will implement, examine and compare the three algorithms and their accuracy in terms of the size of their training set and the number of iterations. {\color{red}(optional: we will also compare them on the computational side, seeing how the performance to computation ratio turns out).} Below we will give a brief introduction to the three algorithms, after briefly introducing \hedge and \weak which form the basis for \adaB and \adaN. A more formal and in depth analysis of the settings and algorithms can be found in chapter \ref{chap:prelim}

\section{\weak}
The idea of boosting is to combine one or more strategies that perform at least slightly better than random guessing, into a good rule to predict an outcome. Such strategies are called weak learners. In this thesis we will primarily consider what are called decision stumps. These weak learners measure one aspect of the input to be classified and then present a decision. Imagine for example that we are trying to decide whether or not an email is spam. An example of a decision stump would be to count the number of \$ in the text and, if there are more than 5 we label it as spam and otherwise as genuine. 

\newpage \section{\hedge}
In their thesis Freund and Schapire introduced \hedge as an allocation algorithm for combining several strategies into one. Consider the following example: When betting on a horse race one might have several rules of thumb. Using \hedge one can distribute one's betting money in such a way that the loss after several rounds will be ``not much worse'' than if one had bet everything according to the best rule of thumb, even though it is usually not known which is the best. This algorithm generalizes previous allocation algorithms as it is not only applicable to multi class classification but also doesn't require any prior knowledge of the bias of the rules of thumb, which was essential to the previous algorithms. A disadvantage of \hedge is however that the losses it achieves depended on a parameter $\eta$ which has to be chosen prior to running the algorithm. While this allows for the exploitation of any prior knowledge one might have it is often not known ahead of time how to choose $\eta$ optimally. Using this \hedge algorithm Freund and Schapire developed \adaB, an application of \hedge in the context of boosting, which we will discuss below. 

\section{\adaB}
\adaB is a learning algorithm, which means that, provided with enough examples with a correct answer attached, \adaB will attempt to formulate an hypothesis, i.e. a rule to label future inputs with, that minimalizes the error made on the new examples. Where \hedge is about deciding between several different strategies, \adaB starts with only one and then generates hypothesis from the examples to form a committee. Further decisions will then be made by a weighted majority vote among this committee. Aside from this one of the most notable differences between the way \hedge and \adaB operate is that \adaB does not require a parameter prior to execution like \hedge does but produces it internally, eliminating more of the need for prior knowledge. Another notable difference is that \hedge increases the weights of the strategies that perform well while \adaB does the exact opposite and thus tries to ``focus'' the weak learning algorithm on the difficult examples. 
