%!TEX root = ../thesis.tex

\chapter{Conclusion and Future Work} 
\label{sec:Concl}

In this thesis we compared the practical performance of the three algorithms \adaB, \NHB and \squintB.  We used both a simulated dataset and a real dataset to compare the algorithms. In the case of the simulated data set we saw a clear hierarchy, namely that \NHB outperformed \adaB which in turn outperformed \squintB. In the setting of the real data set  there we no significant performance differences.
We also illustrated a possible problem with boosting, namely that it might be possible that the data has a few obvious criteria which work fairly well but is very difficult to improve beyond those initial criteria, a property we called the low-hanging fruit property. In this case boosting might not be able to improve beyond a certain point whilst still increasing computation time in both the training and the predicting phase. 

\par Because of time constraints we were only able to compare the algorithms on two data sets. Future work could include the testing over a broader range of data, with both small and large shapes, that may or may not have the low-hanging fruit property. In our hypotheses about the performance of \squintB we assumed that the variance it uses would be smaller than the number of iterations, this need not be the case however. This could be an explanation why \squinB performed worse than the other two, and future work could include investigations on how this variance behaves and how this actually impacts performance. Furthermore it could try to devise certain tests to determine whether the data to be used is conducive to boosting. One of the problems \squintB and \NHB encounter is the problem of tied committees, so further work could include a search to break the ties in a meaningful way to potentially improve the accuracy of the algorithms and to see if this indeed works.