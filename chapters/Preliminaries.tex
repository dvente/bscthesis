%!TEX root = ../thesis.tex

\chapter{Preliminaries}
\label{chap:prelim}
\section{Setting}
\subsection{Classification} 
\label{subsec:class}
We will now provide a formal description of the settings used in this thesis.
\par The first setting is that of \textit{classification} \cite{Hastie2009}. In this setting we are presented with training examples and their correct label. We must then study these examples and come up with a rule to classify future unlabelled inputs. Here we have a finite number of labels lacking any structure such as order or magnitude. We will primarily concern ourselves with binary classification, i.e. results being either negative or positive, since the generalization to multi-class problems has been studied in \cite{Freund1997} and is outside the scope of this thesis. An example of this setting is spam detection. Given enough correctly labelled example emails, we must formulate a rule to label new incoming emails as either genuine or spam. 
\par In this setting we have an input and a label space, denoted $\mathcal X$ and $\mathcal Y$ respectively. For the remainder of this thesis we will generally set $\mathcal X = \R^d$ and $\mathcal Y = \set{0,1}$ or $\mathcal Y = \set{-1,1}$, the latter choice depending on convenience. 
%We will differentiate between generic, or hypothetical, and actually observed data by denoting the former with upper case  and the latter by lower case letters. 
Furthermore vectors will be denoted in boldface and their components will be denoted by a subscript $k$. We will denote our data as follows: $$\Vec{Y_1\\ \mathbf X_1},\ldots, \Vec{Y_i\\ \mathbf X_i},\ldots,\Vec{Y_N\\ \mathbf X_N}$$ where $i$ is the $i$th example ranging from 1 to $N$. Thus, to summarize: $X_{i,k}$ is the $k$th component of the $i$th example $\mathbf X_i$. Finally, since all algorithms are iterative, we will use a superscript $t$ to denote instances at time (or iteration) $t$.

\subsection{Hedging}
\label{subsec:hedging}
In the context of hedging we will have to allocate our resources according to several strategies, in such a way that we minimize our losses. Solutions to hedge settings are often used as subrotines by boosting algorithms to identify important examples and focus on those. An example of this process would be the following: imagine you are betting on a horse race. You have several friends, all of whom advise you according to some rule of thumb and you want to bet a fixed amount on every round. If you knew ahead of time which rule of thumb would be the best, you would bet everything according to that rule but this is usually not known. You try to distribute your betting money in such a way that your losses will not be much worse than if you bet everything according to your luckiest friend's advice.  
\par Formally this means that an agent $A$ has access to $N$ \textit{strategies}, often also called \textit{experts}. It will then attempt to choose a distribution $\vec p^t$ over these strategies at every round, for $T$ rounds. After choosing $\vec p^t$ we will receive a loss vector which we will denote as $\Bell^t$, and suffer loss equal to $$\sum_{i=1}^N p^t_i\ell_i^t = \vec p^t \cdot \Bell^t.$$ In this context we will denote the total loss of the $i$th strategy as $$L_i:= \sum_{t=1}^T \ell^t_i$$ and the total loss of $A$ as $$L_A := \sum_{t=1}^T \vec p^t \cdot \Bell^t.$$ Our goal is to minimize what is called the \textit{regret} which is defined as $$R_A:=L_A - \min_i L_i$$ i.e. the difference in the suffered loss and the minimal loss we could have had. Finally we will sometimes talk about the regret with respect to an expert $i$ which will be denoted as $$R^T_i := \sum^T_{t=1}r_i^t := \sum^T_{t=1} \vec p^t \cdot \Bell^t -\ell_i^t$$

\subsection{Weak learners}
\label{subsec:weak}
In the context of boosting we will require algorithms which are called \textit{weak probably approximately correct (PAC) learning algorithms}\cite{Freund1997}, to which we will refer as \textit{weak learners}. We will furthermore use $\weak$ to denote a generic weak learning algorithm. These weak learners must satisfy the following conditions: given some $\delta,\gamma >0$ and access to enough examples this algorithm must output an hypothesis that has training error at most $\ve \leq \frac12 - \gamma$ with probability $1-\delta$. This means that the algorithm must, with a certain probability (the P in PAC), provide an hypothesis that performs at least slightly better than random guessing (the AC in PAC) on the training set. This low training error will then be expected to generalize to a hopefully low generalization error. The generalization error refers to the error on a set of examples which have their correct labels attached but were not used to train the algorithm but to test it after the training was done. This is to avoid something called \textit{over fitting} and refers to the phenomenon that the rules constructed by the algorithms are overly complicated, which leads to the algorithm's performing very well on the training data but not on new data. 

\par In this thesis the role of the weak learners will mostly be fulfilled by what are called \textit{decision stumps}. These are decision trees of depth one i.e. the learner will measure a single feature of an input and then attempt to formulate an answer. Formally, this means that, given an input $\vec X$ the stump will attempt to find an optimal $r$ and $k$ such that the hypothesis $h:\mathcal X \to \mathcal Y$ given by $$h(\mathbf X):= \begin{cases}1 &\text{if } X_k \leq r\\-1&\text{otherwise}\end{cases}$$ has minimal error on our training set. Here we have chosen to check whether $X_k\leq r$ instead of $X_k\geq r$ but this is another choice for which the stump can optimize. 

\subsection{Boosting}
\label{subsec:boosting}
Recall that the goal of boosting is to take a simple weak learner that only performs slightly better then random guessing and repeatedly assigning weights to the examples so that after training several instances of the weak learner we get a committee which has a significantly better performance than the original learner.
This leads to the following formal setting: we require a set of correctly labelled training data and a weak learning algorithm $\weak$. Often a boosting algorithm can also accept a distribution on the training data to exploit any prior knowledge one might have, which is set to the uniform distribution if no extra knowledge is provided. The procedure at each iteration is to fit \weak to the training data which provides us with an hypothesis $h^t:\mathcal X \to \mathcal Y$. Using this hypothesis we will calculate a loss vector $\Bell^t$. This loss vector is a measure for how well $\weak$ performed on the training data and is usually some form of the training error. For example $\adaB$ uses the weighted error $\vec p^t\cdot\Bell^t$ whereas \NHB also uses a measure called the margin and its distribution, which we will define in section \ref{subsec:NHAlgo}. Using this loss vector we will slightly alter the training set by attaching weights, which measure the relative importance of the examples, and then repeat the process. Our final hypothesis will then be some weighted majority vote among all of the hypotheses. 

\newpage
\section{AdaBoost}
\label{sec:ada}

\begin{algorithm} 
\caption{\adaB}
\label{fig:adaCode}
	\begin{algorithmic}[1]
	\Require 
	\Statex $N$ labelled samples $\Vec{y_1\\ \mathbf x_1},\ldots,\Vec{y_N\\ \mathbf x_N}$  with $\mathcal Y = \set{0,1}$ 
	\Statex Distribution $D$ over the $N$ examples
	\Statex Weak learning algorithm $\weak$
	\Statex Number of trials $T$
	\Procedure{\textbf{AdaBoost}}{}
	\State \textbf{Initialize} the weight vector $w_i^1=D(i)$ for $i=1,\ldots,N$
	\For {$t= 1,2,\ldots,T$}
	\State Set $$\bf p^t= \frac{\bf w^t}{\sum_{i=1}^N w^t_i}$$
	\State Call \weak($\bf p^t$) and receive hypothesis $h^t:\mathcal X\to\mathcal Y$
	\State Calculate the error of $h^t:\ve^t=\sum^N_{i=0}p^t_i|h^t(\mathbf x_i)-y_i|$\label{algStep:adaErr}
	\State Set $\b_t=\ve^t/(1-\ve^t)$
	\State Update the weights $$w_i^{t+1}=w_i^t\beta^{1-|h^t(\vec x_i)-y_i|}_t$$
	\EndFor\\
	\Return $$h_f(\mathbf x)=\begin{cases}1 & \text{if } \sum^T_{t=0}(\log1/\b_t)h^t(\mathbf x)\geq\frac12\sum^T_{t=0}\log1/\b_t\\0&\text{otherwise}\end{cases}$$
	\EndProcedure
	\end{algorithmic}
\end{algorithm}
\subsection{The algorithm}
\label{subsec:AdaAlgo}
We will now give a formal discussion of the \adaB\cite{Freund1997} algorithm, which can be found in algorithm \ref{fig:adaCode} above. The goal of the algorithm is to minimize the error with respect to the distribution 
over the training data. In the first step the learner will obtain a new distribution by normalizing the weights from the previous step, or from the initial distribution. This distribution will then be provided to \weak which will form a hypothesis accordingly. When the hypothesis is formulated the learner will calculate its error and set the parameter $\beta$, which can be interpreted as the learning speed. In the final step the learner will update the weights appropriately. It is important to observe that \weak produces an hypothesis at every iteration and that the final hypothesis is essentially a weighted majority vote among all of these hypotheses. \par Observing the \hedge\cite{Freund1997} algorithm along side \adaB, one can see, most notably in the way the weights are updated, that \hedge is used as a subroutine. This can be seen best by setting $$\ell_i^t := 1-|h^t(\vec x_i)-y_i|$$ and observing that we essentially feed this loss vector to \hedge and use the weights that it produces to identify the important examples. The other algorithms we will examine use constructions similar to this one, which we will discuss in their respective sections.
\newpage
\subsection{The theoretical performance of \adaB}
\label{subsec:AdaTheoPerf}
Here we will discuss the theoretical performance of \adaB. One of the main results from \cite{Freund1997}, is the following theorem: 
\begin{theorem}\label{thm:adaErr}\cite{Freund1997}
Suppose \adaB runs for $T$ rounds calling the weak learning algorithm \weak which generates hypotheses with errors $\ve_1\ldots, \ve_T$ (as defined in step \ref{algStep:adaErr} in algorithm \ref{fig:adaCode}), then the error $\ve:=\Pr_{i\sim D}[h_f(x_i)\neq y_i]$ of the final hypothesis $h_f$ output by \adaB is bounded above by $$\ve\leq 2^T\prod^T_{t=1}\sqrt{\ve_t(1-\ve_t)}$$
\end{theorem}
We will omit the proof, since it is not the focus of this thesis but we will briefly discuss its consequences. As discussed above one might find it strange that every hypothesis is allowed a vote in the final hypothesis. This is still, however, an improvement over previous algorithms since the final error will now depend on the error of all hypotheses instead of only the worst, as was the case in previous works. It is also not immediately obvious that this upper bound does us any good since it contains exponents and products for which it is not trivial how they cancel out. In their paper \cite{Freund1997}, Freund and Shapire show the above stated upper bound  implies to the following one: $$\ve\leq \exp\left(-2\sum^T_{t=0}\gamma_t^2\right)$$ where the $\gamma_t$ is defined such that $\ve_t=1/2-\gamma_t$ holds. This upper bound is more useful since it decreases exponentially, leading us to conclude that \adaB has a good theoretical performance.   


\section{\NHB}
\label{sec:NHB}
\subsection{The algorithm}
\label{subsec:NHAlgo}
In their paper \cite{Luo2014} Luo and Schapire use the framework they had previously developed for drifting game analysis to develop a new algorithm called \adaN which improves upon the previous \textbf{NormalHedge} algorithm. Whereas \textbf{NormalHedge} depends on a numerical search to find the right parameters, Luo and Schapire first develop a general setting for Hedge algorithms which requires convex functions as input and they then proceed to derive a parameter free version which they called \adaN. This algorithm was much more computationally efficient and was easier to generalize to other settings such as boosting. They then proceeded to generalize this to several settings including boosting, which led them to derive the \NHB algorithm which we will discuss here. 
\begin{algorithm} 
\caption{\NHB}
\label{fig:adaNCode}
\begin{algorithmic}[1]
\Require 
\Statex $N$ labelled samples $\Vec{y_1\\ \mathbf x_1},\ldots,\Vec{y_N\\ \mathbf x_N}$  with $\mathcal Y = \set{-1,+1}$ 
\Statex Weak learning algorithm $\weak$
\Statex Number of trials $T$
\Procedure{\adaN}{}
\State \textbf{Initialize} $\mathbf{s}_0 = \mathbf 0$
\For {$t= 1,2,\ldots,T$}
\State Set: $\mathbf p^t \propto \exp([\mathbf s^{t-1}-1]_-^2/3t) - \exp([\mathbf s^{t-1}+1]_-^2/3t) $ 
\State Call $\weak(\mathbf p^t)$ and get hypothesis $h^t$ with edge $\gamma^t = \frac12\sum_{i=1}^Np^t_iy_ih^t(\mathbf x_i)$
\State Set: $\mathbf s^t = \mathbf s^{t-1} + \frac12 y_ih^t(\mathbf x_i)-\gamma^t$
\EndFor\\
\Return $H(\mathbf x) = \sign\lubke{\sum^T_{t=1}h^t(\mathbf{x})}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{The theoretical performance of \NHB}
\label{subsec:NHTheoPerf}
Again \NHB uses several instances of \weak, which are all expected to have \textit{edge} at least $\gamma$, i.e. have error $\frac12 - \gamma$ with respect to the training distribution, to form a committee which decides a final output based on a majority vote. In their paper they formulated and proved the following theorem: 
 \begin{theorem}\label{Thm:NHBPerf}\cite{Luo2014}
 After $T$ rounds the training error of \NHB is of order $\mathcal{O}(\exp(-\frac13T\gamma^2))$ and the fraction of training example with margin at most $\theta(\leq2\gamma)$ is of order $\mathcal{O}(\exp(-\frac13T(\theta-2\gamma)^2))$

 \end{theorem}

 We will again omit the proof but briefly discuss its consequences here. Firstly it is useful to observe that the training error of \NHB is comparable to that of \adaB. However instead of looking only at the training error and trying to generalize that to the generalization error, Luo and Schapiere also make statements with respect to the margin distribution which is a measure of confidence often used in regression settings. The margin of an example with respect to some weak learner is defined as $\frac1T \sum^T_{t=1}yh^t(x)$ i.e. the difference between the fractions of correct and incorrect hypotheses of the example. Here its magnitude measures the confidence of the committee and its sign measures whether or not the weak learner was correct. In this light the above theorem also states that the fraction of training examples the algorithm deems important enough also decreases exponentially and thus it ignores many of the examples every round, making it much more computationally efficient.

 \par As we discussed before \adaB uses \hedge as a subroutine. Similarly \NHB uses \adaN as a subroutine by feeding it a loss vector which is based on the margins of the hypothesis of the previous rounds. So essentially the main difference between \NHB and \adaB is that they use different hedging algorithms as subroutines but the overall (boosting) framework remains the same. This also illustrates that the performance of the boosting algorithms largely depends on the performance of the hedging algorithms that they depend on. Since \hedge and \adaN have similar performances, so do \adaB and \NHB as we will see in chapter \ref{chap:pracPerf}.

\newpage
\section{\squintB}
\label{sec:squintB}
Unlike the previous sections, here we will first take a more careful look at \squint, which is the hedging algorithm instead of the boosting algorithm. This is because \squint hasn't been studied in the context of boosting before and therefore we wish to introduce it formally before we discuss it in the context of boosting. 

\subsection{\squint and second order quantile bounds}
\label{subsec:squint}
Recall that Freund and Shapire proved in their paper \cite{Freund1997} that \hedge can achieve $$R^T_i < \sqrt{T \ln N}\qquad \text{ for each expert } i.$$ While this is a reasonable worst-case performance, one can ask if we can doe better in the common case. Furthermore aside from the performance, the algorithm has another major draw back. This is that this bound increases if one adds experts, even ones that perform well. So even when one adds experts that perform well and one can't eliminate other experts that don't , the theoretical performance of \hedge still decreases. 
In answer to these problems two lines of research were started. The firs one attempted to develop algorithms which perform well with respect to a so called \textit{quantile bound}, i.e. a bound on the regret that considers a subset of experts instead of just one. The second line of research attempted to improve the bounds' dependence on $T$ to a dependence of some form of cumulative variance (or some related second-order quantity) which is shown to be often significantly smaller than $T$. 

\par In response to this research Koolen and Van Erven introduced \squint\cite{Koolen2015} (for Second order quantile integral). This algorithm puts a prior, called $\pi$, on a parameter called the learning rate and on the experts and achieves achieves both a quantile bounds and a variance guarantee. Using the notations $R^T_\mathcal{N} := \mathbb{E}_{\pi(i|\mathcal N)}R^T_i$ and  $V^T_\mathcal{N} := \mathbb{E}_{\pi(i|\mathcal N)}V^T_i$ to denote the average regret under the prior among the reference experts and the uncentered variance of the excess losses respectively with $V^T_i := \sum^T_{t=1}(r^t_i)^2$ and $\mathcal N$ denoting any subset of experts, they formulate the following theorem:

 \begin{theorem}\label{Thm:SquintPerf}\cite{Koolen2015}
 Let $\mathcal{N}$ be any subset of experts and let \squint run for $T$ rounds using the improper prior
  $$w^{t+1}_i \propto \pi(i)\int_0^{\frac12}e^{\eta R^t_i-\eta^2V_i^t} \;d\eta=\pi(i) \frac{\sqrt{\pi}e^{\frac{(R^t_i)^2}{4V^t_i}}\lubke{\erf\lubke{\frac{R^t_i}{2\sqrt{V_i^t}}}-\erf\lubke{\frac{R_i^t-V_i^t}{2\sqrt{V_i^t}}}}}{2\sqrt{V_i^t}}$$
  then the regret of \squint is bounded above by
  $$R^T_\mathcal{N}\leq \sqrt{2V^T_\mathcal N}\lubke{1+\sqrt{2\ln\lubke{\frac{\frac12+\ln(T+1)}{\pi(\mathcal{N})}}}}+5\ln\lubke{1+\frac{1+2\ln(T+1)}{\pi(\mathcal N)}}$$

 \end{theorem}

 Again we will omit the proof but briefly discuss it. Firstly it should be observed that the fact that we have a closed-form for the integral which makes it computationally efficient. Furthermore it is usefully to note that we consider $\ln(\ln(x))$ to be essentially constant so the above bound indeed achieves a quantile bound with a dependence on the variances, improving the bounds of previous algorithms. 
\newpage
\subsection{\squintB}
\label{subsec:algoSquintB}
Plugging the above theorem into the boosting framework yields \squintB wich can be found in algorithm \ref{fig:squintBCode} below. Since there isn't a lot known about \squintB yet we won't go into great detail here, as we will examine it and its perfomance in chapter \ref{chap:pracPerf}
\begin{algorithm} 
\caption{\NHB}
\label{fig:squintBCode}
\begin{algorithmic}[1]
\Require 
\Statex $N$ labelled samples $\Vec{y_1\\ \mathbf x_1},\ldots,\Vec{y_N\\ \mathbf x_N}$  with $\mathcal Y = \set{-1,+1}$ 
\Statex Weak learning algorithm $\weak$
\Statex Number of trials $T$
\Procedure{\squintB}{}
\State \textbf{Initialize} $\mathbf{R^0,V^0,r^0} = \mathbf 0$
\For {$t= 1,2,\ldots,T$}
\State Set: $\mathbf p^t \propto \pi(i) \frac{\sqrt{\pi}e^{\frac{(R^t_i)^2}{4V^t_i}}\lubke{\erf\lubke{\frac{R^t_i}{2\sqrt{V_i^t}}}-\erf\lubke{\frac{R_i^t-V_i^t}{2\sqrt{V_i^t}}}}}{2\sqrt{V_i^t}} $ 
\State Call $\weak(\mathbf p^t)$ and get hypothesis $h^t$ with edge $\gamma^t = \sum_{i=1}^N p^t_ih^t(\mathbf x_i)y_i$
\State Set: $r_i^t=\frac12h^t(\mathbf x_i)y_i-\gamma^t$ 
\State Set: $R_i^t = R_i^{t-1} + r_i^t $ 
\State Set: $V_i^t = V_i^{t-1} + (r_i^t)^2$
\EndFor\\
\Return $H(\mathbf x) = \sign\lubke{\sum^T_{t=1}h^t(\mathbf{x})}$
\EndProcedure
\end{algorithmic}
\end{algorithm}